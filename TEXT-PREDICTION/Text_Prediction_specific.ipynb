{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kz9u2olXtqn"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Activation\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_MA_ocscncX"
      },
      "outputs": [],
      "source": [
        "filepath = \"whatsapp_chat.txt\"\n",
        "file= open(filepath,'r')\n",
        "partial_text=file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mb1ZyHpydV_L"
      },
      "outputs": [],
      "source": [
        "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
        "tokens = tokenizer.tokenize(partial_text.lower())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EP-WY4nd1E1"
      },
      "outputs": [],
      "source": [
        "unique_tokens = np.unique(tokens)\n",
        "unique_token_index = {token : idx for idx , token in enumerate(unique_tokens)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrhwGKUnd_Ao"
      },
      "outputs": [],
      "source": [
        "n_words = 10\n",
        "input_words = []\n",
        "next_words = []\n",
        "for i in range (len(tokens)-n_words) : \n",
        "  input_words.append(tokens[i:i+n_words])\n",
        "  next_words.append(tokens[i+n_words])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCnr4qlOiY2X"
      },
      "outputs": [],
      "source": [
        "X=np.zeros((len(input_words),n_words,len(unique_tokens)),dtype=np.bool_)\n",
        "Y=np.zeros((len(next_words),len(unique_tokens)),dtype=np.bool_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNefNJ-Lj2gi"
      },
      "outputs": [],
      "source": [
        "for i , words in enumerate(input_words):\n",
        "  for j ,  word in enumerate(words):\n",
        "    X[i,j,unique_token_index[word]]=1\n",
        "  Y[i,unique_token_index[next_words[i]]]=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PB0jklFOk5tS"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape = (n_words,len(unique_tokens)),return_sequences=True))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(len(unique_tokens)))\n",
        "model.add(Activation('softmax'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z829PSbVmYxd"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer=RMSprop(learning_rate=0.005),metrics=[\"accuracy\"])\n",
        "model.fit(X,Y,batch_size=128,epochs=25,shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVwJ9vEvrEpK"
      },
      "outputs": [],
      "source": [
        "def predict_next_word(input_text,n_best):\n",
        "  input_text=input_text.lower()\n",
        "  X=np.zeros((1,n_words,len(unique_tokens)))\n",
        "  for i,word in enumerate(input_text.split()):\n",
        "    X[0,i,unique_token_index[word]]=1\n",
        "    \n",
        "  predictions =  model.predict(X)[0]\n",
        "  return np.argpartition(predictions,-n_best)[-n_best:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BcIqjOC3SZA"
      },
      "outputs": [],
      "source": [
        "def generate_text(input_text,text_length,creativity =3):\n",
        "  word_sequence = input_text.split()\n",
        "  current = 0\n",
        "  for _ in range(text_length):\n",
        "    sub_sequence = \" \".join(tokenizer.tokenize(\" \".join(word_sequence).lower())[current:current+n_words])\n",
        "    try : \n",
        "      choice = unique_tokens[random.choice(predict_next_word(sub_sequence,creativity))]\n",
        "    except:\n",
        "      choice = random.choice(unique_tokens)\n",
        "    word_sequence.append(choice)\n",
        "    current+=1\n",
        "  return \" \".join(word_sequence)     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiTwVB715PVF"
      },
      "outputs": [],
      "source": [
        "generate_text(\"Hello\",15,5) #length=15,best 5\n",
        "generate_text(\"Sunday\",10,3) #length =10,best 3"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
